{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5368b2dd-4b49-4abb-ae69-6f79912b1bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "import FinanceDataReader as fdr\n",
    "import requests, lxml\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "\n",
    "import time, datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0a966be1-02b7-406c-b658-372027ad617c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInvesting(ticker):\n",
    "    target_address = 'https://www.google.com/search?q='+ticker+'+investing'\n",
    "    html = requests.get(target_address)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    posts = soup.select('.kCrYT a ')\n",
    "    #for chooes first herf \n",
    "    href = str(posts[0].attrs['href'])\n",
    "    #then href is 'https:/www.investing/equities/stock&~~~~~~ \n",
    "    #so 1.find  start-index : index('equities') , end-index : index('&')\n",
    "    # and slicing href[start : end]\n",
    "    start = href.find('equities')\n",
    "    end = href.find('&')\n",
    "    # print(href[start:end])\n",
    "    return \"https://www.investing.com/\" +href[start:end]+'-commentary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98ebf00a-d187-45f1-be1b-fa8d97049bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(target_address):\n",
    "    result = []\n",
    "    headers = {'User-Agent':\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.61 Safari/537.36\"}\n",
    "    html = requests.get(target_address)\n",
    "    soup = BeautifulSoup(html.text, 'html.parser')\n",
    "    print(soup)\n",
    "    posts = soup.find_all('div', {'class' : 'comment_content__AvzPV'})\n",
    "    for i in posts:\n",
    "        result.append(i.text.strip())\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "cd00ba4e-0e64-490f-8653-7d6316e09390",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = []\n",
    "dates = []\n",
    "\n",
    "def get_title(corp):\n",
    "    \n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_experimental_option('excludeSwitches', ['enable-logging'])\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.5005.61 Safari/537.36')\n",
    "    driver = webdriver.Chrome(executable_path='./chromedriver', options=options)\n",
    "    \n",
    "    driver.get(getInvesting(corp))\n",
    "    driver.implicitly_wait(2)\n",
    "\n",
    "    articles = driver.find_elements_by_class_name('comment_content__AvzPV')\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    \n",
    "    for line in articles:\n",
    "        titles.append(line.text)\n",
    "\n",
    "    for line in soup.select('div.comment_chat-header__36SjM span span'):\n",
    "        title = line.text\n",
    "        dates.append(title)        \n",
    "\n",
    "    page = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    while True:\n",
    "        page += 2\n",
    "        \n",
    "        if page % 100 == 0:\n",
    "            time.sleep(40)\n",
    "        \n",
    "        elif page % 20 == 0:\n",
    "            time.sleep(20)\n",
    "            \n",
    "        elif page % 10 == 0:\n",
    "            time.sleep(10)\n",
    "            \n",
    "        elif page % 5 == 0:\n",
    "            time.sleep(7)\n",
    "        \n",
    "        url = getInvesting(corp)+ '/' + str(page)\n",
    "        \n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(10)\n",
    "\n",
    "        articles = driver.find_elements_by_class_name('comment_content__AvzPV')\n",
    "        soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "        link = soup.find_all('link', rel='canonical')\n",
    "        \n",
    "        if link[0]['href'] == getInvesting(corp):\n",
    "            print(cnt)\n",
    "            \n",
    "            break\n",
    "\n",
    "        for line in articles:\n",
    "            titles.append(line.text)\n",
    "            \n",
    "        for line in soup.select('div.comment_chat-header__36SjM span span'):\n",
    "            title = line.text\n",
    "            dates.append(title)\n",
    "        \n",
    "        cnt += 1\n",
    "        \n",
    "    driver.close()\n",
    "\n",
    "    return titles, dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "220168c8-93dc-4600-a124-942fb470c62f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xw/rbcyfp692g3c5w_0qcls7ncm0000gn/T/ipykernel_21490/2397454304.py:9: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(executable_path='./chromedriver', options=options)\n",
      "/var/folders/xw/rbcyfp692g3c5w_0qcls7ncm0000gn/T/ipykernel_21490/2397454304.py:14: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  articles = driver.find_elements_by_class_name('comment_content__AvzPV')\n",
      "/var/folders/xw/rbcyfp692g3c5w_0qcls7ncm0000gn/T/ipykernel_21490/2397454304.py:44: DeprecationWarning: find_elements_by_class_name is deprecated. Please use find_elements(by=By.CLASS_NAME, value=name) instead\n",
      "  articles = driver.find_elements_by_class_name('comment_content__AvzPV')\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [338]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mget_title\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTSLA\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDATE\u001b[39m\u001b[38;5;124m'\u001b[39m : result[\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCOMMENT\u001b[39m\u001b[38;5;124m'\u001b[39m : result[\u001b[38;5;241m0\u001b[39m]})\n\u001b[1;32m      7\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTESLA.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUTF-8\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Input \u001b[0;32mIn [337]\u001b[0m, in \u001b[0;36mget_title\u001b[0;34m(corp)\u001b[0m\n\u001b[1;32m     45\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(driver\u001b[38;5;241m.\u001b[39mpage_source, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlxml\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     46\u001b[0m link \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlink\u001b[39m\u001b[38;5;124m'\u001b[39m, rel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcanonical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlink\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m getInvesting(corp):\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28mprint\u001b[39m(cnt)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "result = get_title('TSLA')\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    {'DATE' : result[1],\n",
    "    'COMMENT' : result[0]})\n",
    "\n",
    "df.to_csv('TESLA.csv', index=False, encoding='UTF-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
